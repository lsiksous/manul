

@article{syntetos01_bias_inter_deman_estim,
  author =       {A.A Syntetos and J.E Boylan},
  title =        {On the Bias of Intermittent Demand Estimates},
  journal =      {International Journal of Production Economics},
  volume =       71,
  number =       {1-3},
  pages =        {457-466},
  year =         2001,
  doi =          {10.1016/s0925-5273(00)00143-2},
  url =          {http://dx.doi.org/10.1016/S0925-5273(00)00143-2},
  DATE_ADDED =   {Wed Feb 15 10:11:33 2023},
}


@article{vinhForecastingIrregularDemand,
  author =       {Dang Quang Vinh},
  title =        {Forecasting irregular demand for spare parts inventory},
  journal =      {},
  volume =       ,
  number =       {},
  pages =        {},
  year =         2005,
  doi =          {},
  url =          {https://1library.net/document/yn0el0lq-forecasting-irregular-demand-for-spare-parts-inventory.html},
  DATE_ADDED =   {Wed Feb 15 10:11:33 2023},
}

@article{croston72_forec_stock_contr_inter_deman,
  author =       {J. D. Croston},
  title =        {Forecasting and Stock Control for Intermittent Demands},
  journal =      {Journal of the Operational Research Society},
  volume =       23,
  number =       3,
  pages =        {289-303},
  year =         1972,
  doi =          {10.1057/jors.1972.50},
  url =          {http://dx.doi.org/10.1057/jors.1972.50},
  DATE_ADDED =   {Wed Feb 15 10:25:40 2023},
}


@inproceedings{Ke2017LightGBMAH,
  title={LightGBM: A Highly Efficient Gradient Boosting Decision Tree},
  author={Guolin Ke and Qi Meng and Thomas Finley and Taifeng Wang and Wei Chen and Weidong Ma and Qiwei Ye and Tie-Yan Liu},
  booktitle={NIPS},
  year={2017}
}

@misc{taylor17_forec,
  DATE_ADDED =   {Wed Feb 15 11:16:16 2023},
  author =       {Sean J Taylor and Benjamin Letham},
  doi =          {10.7287/peerj.preprints.3190v2},
  title =        {Forecasting at scale},
  url =          {http://dx.doi.org/10.7287/peerj.preprints.3190v2},
  year =         2017,
}

@inproceedings{geng19_light,
  author =       {Yangli-ao Geng and Qingyong Li and Tianyang Lin and Lei Jiang
                  and Liangtao Xu and Dong Zheng and Wen Yao and Weitao Lyu and
                  Yijun Zhang},
  title =        {LightNet},
  booktitle =    {Proceedings of the 25th ACM SIGKDD International Conference on
                  Knowledge Discovery &amp; Data Mining},
  year =         2019,
  pages =        {nil},
  doi =          {10.1145/3292500.3330717},
  url =          {http://dx.doi.org/10.1145/3292500.3330717},
  DATE_ADDED =   {Wed Feb 15 11:17:39 2023},
  month =        7,
}

@article{salinas17_deepar,
  author =       {David Salinas and Valentin Flunkert and Jan Gasthaus},
  title =        {Deepar: Probabilistic Forecasting With Autoregressive
                  Recurrent Networks},
  journal =      {nil},
  volume =       {nil},
  number =       {nil},
  pages =        {nil},
  year =         2017,
  doi =          {10.48550/ARXIV.1704.04110},
  url =          {https://arxiv.org/abs/1704.04110},
  DATE_ADDED =   {Wed Feb 15 11:21:27 2023},
}

@article{lim19_tempor_fusion_trans_inter_multi,
  author =       {Bryan Lim and Sercan O. Arik and Nicolas Loeff and Tomas
                  Pfister},
  title =        {Temporal Fusion Transformers for Interpretable Multi-Horizon
                  Time Series Forecasting},
  journal =      {nil},
  volume =       {nil},
  number =       {nil},
  pages =        {nil},
  year =         2019,
  doi =          {10.48550/ARXIV.1912.09363},
  url =          {https://arxiv.org/abs/1912.09363},
  DATE_ADDED =   {Wed Feb 15 11:27:34 2023},
}

@article{oreshkin19_n_beats,
  author =       {Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and
                  Yoshua Bengio},
  title =        {N-Beats: Neural Basis Expansion Analysis for Interpretable
                  Time Series Forecasting},
  journal =      {nil},
  volume =       {nil},
  number =       {nil},
  pages =        {nil},
  year =         2019,
  doi =          {10.48550/ARXIV.1905.10437},
  url =          {https://arxiv.org/abs/1905.10437},
  DATE_ADDED =   {Wed Feb 15 11:34:05 2023},
}

@article{challu22_n_hits,
  author =       {Cristian Challu and Kin G. Olivares and Boris N. Oreshkin and
                  Federico Garza and Max Mergenthaler-Canseco and Artur
                  Dubrawski},
  title =        {N-Hits: Neural Hierarchical Interpolation for Time Series
                  Forecasting},
  journal =      {nil},
  volume =       {nil},
  number =       {nil},
  pages =        {nil},
  year =         2022,
  doi =          {10.48550/ARXIV.2201.12886},
  url =          {https://arxiv.org/abs/2201.12886},
  DATE_ADDED =   {Wed Feb 15 11:36:35 2023},
}

@article{kaya20_inter_deman_forec,
  author =       {Gamze Ogcu Kaya and Merve Sahin and Omer Fahrettin Demirel},
  title =        {Intermittent Demand Forecasting: a Guideline for Method
                  Selection},
  journal =      {Sādhanā},
  volume =       45,
  number =       1,
  pages =        51,
  year =         2020,
  doi =          {10.1007/s12046-020-1285-8},
  url =          {http://dx.doi.org/10.1007/s12046-020-1285-8},
  DATE_ADDED =   {Wed Feb 15 12:16:24 2023},
}

@article{calinski74_dendr_method_clust_analy,
  author =       {T. Calinski and J. Harabasz},
  title =        {A Dendrite Method for Cluster Analysis},
  journal =      {Communications in Statistics - Theory and Methods},
  volume =       3,
  number =       1,
  pages =        {1-27},
  year =         1974,
  doi =          {10.1080/03610927408827101},
  url =          {http://dx.doi.org/10.1080/03610927408827101},
  DATE_ADDED =   {Wed Feb 15 12:35:07 2023},
}

@article{chen20_time_series_clust_class,
  author =       {Ming Chen},
  title =        {Time Series Clustering and Classification},
  journal =      {Journal of the American Statistical Association},
  volume =       115,
  number =       531,
  pages =        {1558-1558},
  year =         2020,
  doi =          {10.1080/01621459.2020.1801281},
  url =          {http://dx.doi.org/10.1080/01621459.2020.1801281},
  DATE_ADDED =   {Wed Feb 15 16:21:14 2023},
}

@misc{angeliniCrackingNutsSledgehammer2022,
  title = {Cracking Nuts with a Sledgehammer: When Modern Graph Neural Networks Do Worse than Classical Greedy Algorithms},
  shorttitle = {Cracking Nuts with a Sledgehammer},
  author = {Angelini, Maria Chiara and Ricci-Tersenghi, Federico},
  date = {2022-06-27},
  number = {arXiv:2206.13211},
  eprint = {2206.13211},
  eprinttype = {arxiv},
  primaryclass = {cond-mat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.13211},
  url = {http://arxiv.org/abs/2206.13211},
  urldate = {2022-08-09},
  abstract = {The recent work ``Combinatorial Optimization with Physics-Inspired Graph Neural Networks'' [Nat Mach Intell 4 (2022) 367] introduces a physics-inspired unsupervised Graph Neural Network (GNN) to solve combinatorial optimization problems on sparse graphs. To test the performances of these GNNs, the authors of the work show numerical results for two fundamental problems: maximum cut and maximum independent set (MIS). They conclude that "the graph neural network optimizer performs on par or outperforms existing solvers, with the ability to scale beyond the state of the art to problems with millions of variables." In this comment, we show that a simple greedy algorithm, running in almost linear time, can find solutions for the MIS problem of much better quality than the GNN. The greedy algorithm is faster by a factor of \$10\^4\$ with respect to the GNN for problems with a million variables. We do not see any good reason for solving the MIS with these GNN, as well as for using a sledgehammer to crack nuts. In general, many claims of superiority of neural networks in solving combinatorial problems are at risk of being not solid enough, since we lack standard benchmarks based on really hard problems. We propose one of such hard benchmarks, and we hope to see future neural network optimizers tested on these problems before any claim of superiority is made.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Mathematics - Optimization and Control},
  file = {/Users/lss/Zotero/storage/BJM8F4BU/Angelini and Ricci-Tersenghi - 2022 - Cracking nuts with a sledgehammer when modern gra.pdf;/Users/lss/Zotero/storage/XJZQCD4E/2206.html}
}

@article{assimakopoulosThetaModelDecomposition2000,
  title = {The Theta Model: {{A}} Decomposition Approach to Forecasting},
  shorttitle = {The Theta Model},
  author = {Assimakopoulos, Vassilis and Nikolopoulos, K.},
  date = {2000-10-01},
  journaltitle = {International Journal of Forecasting},
  shortjournal = {International Journal of Forecasting},
  volume = {16},
  pages = {521--530},
  doi = {10.1016/S0169-2070(00)00066-2},
  abstract = {This paper presents a new univariate forecasting method. The method is based on the concept of modifying the local curvature of the time-series through a coefficient ‘Theta’ (the Greek letter θ), that is applied directly to the second differences of the data. The resulting series that are created maintain the mean and the slope of the original data but not their curvatures. These new time series are named Theta-lines. Their primary qualitative characteristic is the improvement of the approximation of the long-term behavior of the data or the augmentation of the short-term features, depending on the value of the Theta coefficient. The proposed method decomposes the original time series into two or more different Theta-lines. These are extrapolated separately and the subsequent forecasts are combined. The simple combination of two Theta-lines, the Theta=0 (straight line) and Theta=2 (double local curves) was adopted in order to produce forecasts for the 3003 series of the M3 competition. The method performed well, particularly for monthly series and for microeconomic data.},
  file = {/Users/lss/Zotero/storage/JBFEHJNY/Assimakopoulos and Nikolopoulos - 2000 - The theta model A decomposition approach to forec.pdf}
}

@article{bagnallGreatTimeSeries2017,
  title = {The Great Time Series Classification Bake off: A Review and Experimental Evaluation of Recent Algorithmic Advances},
  shorttitle = {The Great Time Series Classification Bake Off},
  author = {Bagnall, Anthony and Lines, Jason and Bostrom, Aaron and Large, James and Keogh, Eamonn},
  date = {2017-05-01},
  journaltitle = {Data Mining and Knowledge Discovery},
  shortjournal = {Data Min Knowl Disc},
  volume = {31},
  number = {3},
  pages = {606--660},
  issn = {1573-756X},
  doi = {10.1007/s10618-016-0483-9},
  url = {https://doi.org/10.1007/s10618-016-0483-9},
  urldate = {2022-11-17},
  abstract = {In the last 5~years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only nine of these algorithms are significantly more accurate than both benchmarks and that one classifier, the collective of transformation ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more robust testing of new algorithms in the future.},
  langid = {english},
  keywords = {Elastic distance measures,Shapelets,Time series classification,Time series similarity},
  file = {/Users/lss/Zotero/storage/YNFJ6UKE/Bagnall et al. - 2017 - The great time series classification bake off a r.pdf}
}

@misc{caoSpectralTemporalGraph2021,
  title = {Spectral {{Temporal Graph Neural Network}} for {{Multivariate Time-series Forecasting}}},
  author = {Cao, Defu and Wang, Yujing and Duan, Juanyong and Zhang, Ce and Zhu, Xia and Huang, Conguri and Tong, Yunhai and Xu, Bixiong and Bai, Jing and Tong, Jie and Zhang, Qi},
  date = {2021-03-13},
  number = {arXiv:2103.07719},
  eprint = {2103.07719},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.07719},
  url = {http://arxiv.org/abs/2103.07719},
  urldate = {2022-07-25},
  abstract = {Multivariate time-series forecasting plays a crucial role in many real-world applications. It is a challenging problem as one needs to consider both intra-series temporal correlations and inter-series correlations simultaneously. Recently, there have been multiple works trying to capture both correlations, but most, if not all of them only capture temporal correlations in the time domain and resort to pre-defined priors as inter-series relationships. In this paper, we propose Spectral Temporal Graph Neural Network (StemGNN) to further improve the accuracy of multivariate time-series forecasting. StemGNN captures inter-series correlations and temporal dependencies \textbackslash textit\{jointly\} in the \textbackslash textit\{spectral domain\}. It combines Graph Fourier Transform (GFT) which models inter-series correlations and Discrete Fourier Transform (DFT) which models temporal dependencies in an end-to-end framework. After passing through GFT and DFT, the spectral representations hold clear patterns and can be predicted effectively by convolution and sequential learning modules. Moreover, StemGNN learns inter-series correlations automatically from the data without using pre-defined priors. We conduct extensive experiments on ten real-world datasets to demonstrate the effectiveness of StemGNN. Code is available at https://github.com/microsoft/StemGNN/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/lss/Zotero/storage/ETTUHQFC/Cao et al. - 2021 - Spectral Temporal Graph Neural Network for Multiva.pdf;/Users/lss/Zotero/storage/DHPJS4FW/2103.html}
}

@online{cerlianiAnomalyDetectionMultivariate2020,
  title = {Anomaly {{Detection}} in {{Multivariate Time Series}} with {{VAR}}},
  author = {Cerliani, Marco},
  date = {2020-06-17T16:24:42},
  url = {https://towardsdatascience.com/anomaly-detection-in-multivariate-time-series-with-var-2130f276e5e9},
  urldate = {2022-08-24},
  abstract = {System Monitoring in Presence of Serial Correlation},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/lss/Zotero/storage/SCY3GSFS/anomaly-detection-in-multivariate-time-series-with-var-2130f276e5e9.html}
}

@unpublished{challuNHiTSNeuralHierarchical2022,
  title = {N-{{HiTS}}: {{Neural Hierarchical Interpolation}} for {{Time Series Forecasting}}},
  shorttitle = {N-{{HiTS}}},
  author = {Challu, Cristian and Olivares, Kin G. and Oreshkin, Boris N. and Garza, Federico and Mergenthaler-Canseco, Max and Dubrawski, Artur},
  date = {2022-05-28},
  number = {arXiv:2201.12886},
  eprint = {2201.12886},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.12886},
  url = {http://arxiv.org/abs/2201.12886},
  urldate = {2022-06-03},
  abstract = {Recent progress in neural forecasting accelerated improvements in the performance of large-scale forecasting systems. Yet, long-horizon forecasting remains a very difficult task. Two common challenges afflicting the task are the volatility of the predictions and their computational complexity. We introduce N-HiTS, a model which addresses both challenges by incorporating novel hierarchical interpolation and multi-rate data sampling techniques. These techniques enable the proposed method to assemble its predictions sequentially, emphasizing components with different frequencies and scales while decomposing the input signal and synthesizing the forecast. We prove that the hierarchical interpolation technique can efficiently approximate arbitrarily long horizons in the presence of smoothness. Additionally, we conduct extensive large-scale dataset experiments from the long-horizon forecast literature, demonstrating the advantages of our method over the state-of-the-art methods, where N-HiTS provides an average accuracy improvement of 25\% over the latest Transformer architectures while reducing the computation time by an order of magnitude (50 times).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/lss/Zotero/storage/JTTM3DA7/Challu et al. - 2022 - N-HiTS Neural Hierarchical Interpolation for Time.pdf;/Users/lss/Zotero/storage/H4MS54J7/2201.html}
}

@online{citationsyPDF106029,
  title = {[{{PDF}}] 10.6029/Smartcr.2014.03.007},
  author = {Citationsy},
  url = {https://citationsy.com/archives/q?doi=10.6029/smartcr.2014.03.007},
  urldate = {2022-06-13},
  abstract = {Download and reference 10.6029/smartcr.2014.03.007},
  langid = {english},
  organization = {{Citationsy}},
  file = {/Users/lss/Zotero/storage/RS9NNDSH/q.html}
}

@misc{dorogushCatBoostGradientBoosting2018,
  title = {{{CatBoost}}: Gradient Boosting with Categorical Features Support},
  shorttitle = {{{CatBoost}}},
  author = {Dorogush, Anna Veronika and Ershov, Vasily and Gulin, Andrey},
  date = {2018-10-24},
  number = {arXiv:1810.11363},
  eprint = {1810.11363},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.11363},
  url = {http://arxiv.org/abs/1810.11363},
  urldate = {2023-01-17},
  abstract = {In this paper we present CatBoost, a new open-sourced gradient boosting library that successfully handles categorical features and outperforms existing publicly available implementations of gradient boosting in terms of quality on a set of popular publicly available datasets. The library has a GPU implementation of learning algorithm and a CPU implementation of scoring algorithm, which are significantly faster than other gradient boosting libraries on ensembles of similar sizes.},
  archiveprefix = {arXiv},
  version = {1},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Statistics - Machine Learning},
  file = {/Users/lss/Zotero/storage/95AAMDWE/Dorogush et al. - 2018 - CatBoost gradient boosting with categorical featu.pdf;/Users/lss/Zotero/storage/CLGKH5CA/1810.html}
}

@unpublished{gangopadhyaySpatiotemporalAttentionMultivariate2020,
  title = {Spatiotemporal {{Attention}} for {{Multivariate Time Series Prediction}} and {{Interpretation}}},
  author = {Gangopadhyay, Tryambak and Tan, Sin Yong and Jiang, Zhanhong and Meng, Rui and Sarkar, Soumik},
  date = {2020-10-26},
  number = {arXiv:2008.04882},
  eprint = {2008.04882},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2008.04882},
  url = {http://arxiv.org/abs/2008.04882},
  urldate = {2022-06-03},
  abstract = {Multivariate time series modeling and prediction problems are abundant in many machine learning application domains. Accurate interpretation of such prediction outcomes from a machine learning model that explicitly captures temporal correlations can significantly benefit the domain experts. In this context, temporal attention has been successfully applied to isolate the important time steps for the input time series. However, in multivariate time series problems, spatial interpretation is also critical to understand the contributions of different variables on the model outputs. We propose a novel deep learning architecture, called spatiotemporal attention mechanism (STAM) for simultaneous learning of the most important time steps and variables. STAM is a causal (i.e., only depends on past inputs and does not use future inputs) and scalable (i.e., scales well with an increase in the number of variables) approach that is comparable to the state-of-the-art models in terms of computational tractability. We demonstrate our models' performance on two popular public datasets and a domain-specific dataset. When compared with the baseline models, the results show that STAM maintains state-of-the-art prediction accuracy while offering the benefit of accurate spatiotemporal interpretability. The learned attention weights are validated from a domain knowledge perspective for these real-world datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lss/Zotero/storage/LLCTAY2U/Gangopadhyay et al. - 2020 - Spatiotemporal Attention for Multivariate Time Ser.pdf;/Users/lss/Zotero/storage/QK93RVYN/2008.html}
}

@misc{grigsbyLongRangeTransformersDynamic2022,
  title = {Long-{{Range Transformers}} for {{Dynamic Spatiotemporal Forecasting}}},
  author = {Grigsby, Jake and Wang, Zhe and Qi, Yanjun},
  date = {2022-05-19},
  number = {arXiv:2109.12218},
  eprint = {2109.12218},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.12218},
  url = {http://arxiv.org/abs/2109.12218},
  urldate = {2022-10-05},
  abstract = {Multivariate Time Series Forecasting focuses on the prediction of future values based on historical context. State-of-the-art sequence-to-sequence models rely on neural attention between timesteps, which allows for temporal learning but fails to consider distinct spatial relationships between variables. In contrast, methods based on graph neural networks explicitly model variable relationships. However, these methods often rely on predefined graphs and perform separate spatial and temporal updates without establishing direct connections between each variable at every timestep. This paper addresses these problems by translating multivariate forecasting into a spatiotemporal sequence formulation where each Transformer input token represents the value of a single variable at a given time. Long-Range Transformers can then learn interactions between space, time, and value information jointly along this extended sequence. Our method, which we call Spacetimeformer, achieves competitive results on benchmarks from traffic forecasting to electricity demand and weather prediction while learning fully-connected spatiotemporal relationships purely from data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lss/Zotero/storage/6559XV7K/Grigsby et al. - 2022 - Long-Range Transformers for Dynamic Spatiotemporal.pdf;/Users/lss/Zotero/storage/TE52UWNB/2109.html}
}

@unpublished{hosseiniFlexibleForecastingModel2021,
  title = {A Flexible Forecasting Model for Production Systems},
  author = {Hosseini, Reza and Yang, Kaixu and Chen, Albert and Patra, Sayan},
  date = {2021-05-03},
  number = {arXiv:2105.01098},
  eprint = {2105.01098},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.01098},
  url = {http://arxiv.org/abs/2105.01098},
  urldate = {2022-06-03},
  abstract = {This paper discusses desirable properties of forecasting models in production systems. It then develops a family of models which are designed to satisfy these properties: highly customizable to capture complex patterns; accommodates a large variety of objectives; has interpretable components; produces robust results; has automatic changepoint detection for trend and seasonality; and runs fast -- making it a good choice for reliable and scalable production systems. The model allows for seasonality at various time scales, events/holidays, and change points in trend and seasonality. The volatility is fitted separately to maintain flexibility and speed and is allowed to be a function of specified features.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/lss/Zotero/storage/S6A93U5Y/Hosseini et al. - 2021 - A flexible forecasting model for production system.pdf;/Users/lss/Zotero/storage/SBH8D6L5/2105.html}
}

@misc{hosseiniFlexibleForecastingModel2021a,
  title = {A Flexible Forecasting Model for Production Systems},
  author = {Hosseini, Reza and Yang, Kaixu and Chen, Albert and Patra, Sayan},
  date = {2021-05-03},
  number = {arXiv:2105.01098},
  eprint = {2105.01098},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.01098},
  url = {http://arxiv.org/abs/2105.01098},
  urldate = {2022-10-13},
  abstract = {This paper discusses desirable properties of forecasting models in production systems. It then develops a family of models which are designed to satisfy these properties: highly customizable to capture complex patterns; accommodates a large variety of objectives; has interpretable components; produces robust results; has automatic changepoint detection for trend and seasonality; and runs fast -- making it a good choice for reliable and scalable production systems. The model allows for seasonality at various time scales, events/holidays, and change points in trend and seasonality. The volatility is fitted separately to maintain flexibility and speed and is allowed to be a function of specified features.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/lss/Zotero/storage/9RDEM7XV/Hosseini et al. - 2021 - A flexible forecasting model for production system.pdf;/Users/lss/Zotero/storage/9I8FQ2V7/2105.html}
}

@online{jepsenHowDeepLearning2019,
  title = {How to Do {{Deep Learning}} on {{Graphs}} with {{Graph Convolutional Networks}}},
  author = {Jepsen, Tobias Skovgaard},
  date = {2019-05-07T16:57:38},
  url = {https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780},
  urldate = {2022-03-08},
  abstract = {Part 1: A High-Level Introduction to Graph Convolutional Networks},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/lss/Zotero/storage/BP3XPXBI/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780.html}
}

@article{kumarFeatureSelectionLiterature2014,
  title = {Feature {{Selection}}: {{A}} Literature {{Review}}},
  shorttitle = {Feature {{Selection}}},
  author = {Kumar, Vipin},
  date = {2014-06-30},
  journaltitle = {The Smart Computing Review},
  shortjournal = {SmartCR},
  volume = {4},
  number = {3},
  issn = {22344624},
  doi = {10.6029/smartcr.2014.03.007},
  url = {http://www.smartcr.org/view/download.php?filename=smartcr_vol4no3p7.pdf},
  urldate = {2022-06-13},
  abstract = {Relevant feature identification has become an essential task to apply data mining algorithms effectively in real-world scenarios. Therefore, many feature selection methods have been proposed to obtain the relevant feature or feature subsets in the literature to achieve their objectives of classification and clustering. This paper introduces the concepts of feature relevance, general procedures, evaluation criteria, and the characteristics of feature selection. A comprehensive overview, categorization, and comparison of existing feature selection methods are also done, and the guidelines are also provided for user to select a feature selection algorithm without knowing the information of each algorithm. We conclude this work with real world applications, challenges, and future research directions of feature selection.},
  langid = {english},
  file = {/Users/lss/Zotero/storage/U2DEN339/Kumar - 2014 - Feature Selection A literature Review.pdf}
}

@article{lecuyerLocalisationDynamiqueElevages2018,
  title = {Localisation et dynamique des élevages et des abattoirs – Analyse des données BDPORC},
  author = {Lécuyer, Bérengère},
  date = {2018},
  pages = {66},
  langid = {french},
  file = {/Users/lss/Zotero/storage/579H5ZIR/Lécuyer - 2018 - Localisation et dynamique des élevages et des abat.pdf}
}

@unpublished{limTemporalFusionTransformers2020,
  title = {Temporal {{Fusion Transformers}} for {{Interpretable Multi-horizon Time Series Forecasting}}},
  author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
  date = {2020-09-27},
  number = {arXiv:1912.09363},
  eprint = {1912.09363},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.09363},
  url = {http://arxiv.org/abs/1912.09363},
  urldate = {2022-10-03},
  abstract = {Multi-horizon forecasting problems often contain a complex mix of inputs -- including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed historically -- without any prior information on how they interact with the target. While several deep learning models have been proposed for multi-step prediction, they typically comprise black-box models which do not account for the full range of inputs present in common scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) -- a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, the TFT utilizes recurrent layers for local processing and interpretable self-attention layers for learning long-term dependencies. The TFT also uses specialized components for the judicious selection of relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of regimes. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use-cases of TFT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lss/Zotero/storage/4XEVYQKE/Lim et al. - 2020 - Temporal Fusion Transformers for Interpretable Mul.pdf;/Users/lss/Zotero/storage/3T56FYH4/1912.html}
}

@unpublished{liuDSTPRNNDualstageTwophase2019,
  title = {{{DSTP-RNN}}: A Dual-Stage Two-Phase Attention-Based Recurrent Neural Networks for Long-Term and Multivariate Time Series Prediction},
  shorttitle = {{{DSTP-RNN}}},
  author = {Liu, Yeqi and Gong, Chuanyang and Yang, Ling and Chen, Yingyi},
  date = {2019-04-16},
  number = {arXiv:1904.07464},
  eprint = {1904.07464},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.07464},
  url = {http://arxiv.org/abs/1904.07464},
  urldate = {2022-10-03},
  abstract = {Long-term prediction of multivariate time series is still an important but challenging problem. The key to solve this problem is to capture the spatial correlations at the same time, the spatio-temporal relationships at different times and the long-term dependence of the temporal relationships between different series. Attention-based recurrent neural networks (RNN) can effectively represent the dynamic spatio-temporal relationships between exogenous series and target series, but it only performs well in one-step time prediction and short-term time prediction. In this paper, inspired by human attention mechanism including the dual-stage two-phase (DSTP) model and the influence mechanism of target information and non-target information, we propose DSTP-based RNN (DSTP-RNN) and DSTP-RNN-2 respectively for long-term time series prediction. Specifically, we first propose the DSTP-based structure to enhance the spatial correlations between exogenous series. The first phase produces violent but decentralized response weight, while the second phase leads to stationary and concentrated response weight. Secondly, we employ multiple attentions on target series to boost the long-term dependence. Finally, we study the performance of deep spatial attention mechanism and provide experiment and interpretation. Our methods outperform nine baseline methods on four datasets in the fields of energy, finance, environment and medicine, respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lss/Zotero/storage/K2EGKZCT/Liu et al. - 2019 - DSTP-RNN a dual-stage two-phase attention-based r.pdf;/Users/lss/Zotero/storage/Q5PXKHGU/1904.html}
}

@book{maharajTimeSeriesClustering2019,
  title = {Time Series Clustering and Classification},
  author = {Maharaj, Elizabeth Ann and D'Urso, Pierpaolo and Caiado, Jorge},
  date = {2019},
  series = {Chapman \& {{Hall}}/{{CRC}} Computer Science and Data Analysis Series},
  publisher = {{CRC Press, Taylor \& Francis Group}},
  location = {{Boca Raton}},
  isbn = {978-0-429-05826-4 978-0-429-60330-3},
  pagetotal = {1},
  keywords = {Cluster analysis,Time-series analysis},
  file = {/Users/lss/Zotero/storage/M63F79M9/Maharaj et al. - 2019 - Time series clustering and classification.pdf}
}

@book{MementoEleveurPorc2013a,
  title = {Mémento de l'éleveur de porc},
  date = {2013},
  edition = {7e éd},
  publisher = {{IFIP-Institut du porc}},
  location = {{Paris}},
  url = {https://ifip.asso.fr/documentations/4610-memento-de-leleveur-de-porc-edition-2013/},
  isbn = {978-2-85969-226-1},
  langid = {fre},
  annotation = {OCLC: 881421341}
}

@unpublished{oreshkinNBEATSNeuralBasis2020,
  title = {N-{{BEATS}}: {{Neural}} Basis Expansion Analysis for Interpretable Time Series Forecasting},
  shorttitle = {N-{{BEATS}}},
  author = {Oreshkin, Boris N. and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
  date = {2020-02-20},
  number = {arXiv:1905.10437},
  eprint = {1905.10437},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.10437},
  url = {http://arxiv.org/abs/1905.10437},
  urldate = {2022-06-03},
  abstract = {We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11\% over a statistical benchmark and by 3\% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lss/Zotero/storage/23UCMRDG/Oreshkin et al. - 2020 - N-BEATS Neural basis expansion analysis for inter.pdf;/Users/lss/Zotero/storage/SASS2ABB/1905.html}
}

@misc{prokhorenkovaCatBoostUnbiasedBoosting2019,
  title = {{{CatBoost}}: Unbiased Boosting with Categorical Features},
  shorttitle = {{{CatBoost}}},
  author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
  date = {2019-01-20},
  number = {arXiv:1706.09516},
  eprint = {1706.09516},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.09516},
  url = {http://arxiv.org/abs/1706.09516},
  urldate = {2023-01-17},
  abstract = {This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/lss/Zotero/storage/7SRUZNPX/Prokhorenkova et al. - 2019 - CatBoost unbiased boosting with categorical featu.pdf;/Users/lss/Zotero/storage/I26TBQUW/1706.html}
}

@unpublished{rozemberczkiPyTorchGeometricTemporal2021,
  title = {{{PyTorch Geometric Temporal}}: {{Spatiotemporal Signal Processing}} with {{Neural Machine Learning Models}}},
  shorttitle = {{{PyTorch Geometric Temporal}}},
  author = {Rozemberczki, Benedek and Scherer, Paul and He, Yixuan and Panagopoulos, George and Riedel, Alexander and Astefanoaei, Maria and Kiss, Oliver and Beres, Ferenc and López, Guzmán and Collignon, Nicolas and Sarkar, Rik},
  date = {2021-06-10},
  number = {arXiv:2104.07788},
  eprint = {2104.07788},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.07788},
  url = {http://arxiv.org/abs/2104.07788},
  urldate = {2022-09-09},
  abstract = {We present PyTorch Geometric Temporal a deep learning framework combining state-of-the-art machine learning algorithms for neural spatiotemporal signal processing. The main goal of the library is to make temporal geometric deep learning available for researchers and machine learning practitioners in a unified easy-to-use framework. PyTorch Geometric Temporal was created with foundations on existing libraries in the PyTorch eco-system, streamlined neural network layer definitions, temporal snapshot generators for batching, and integrated benchmark datasets. These features are illustrated with a tutorial-like case study. Experiments demonstrate the predictive performance of the models implemented in the library on real world problems such as epidemiological forecasting, ridehail demand prediction and web-traffic management. Our sensitivity analysis of runtime shows that the framework can potentially operate on web-scale datasets with rich temporal features and spatial structure.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/lss/Zotero/storage/6H462DC3/Rozemberczki et al. - 2021 - PyTorch Geometric Temporal Spatiotemporal Signal .pdf;/Users/lss/Zotero/storage/QXXNB96S/2104.html}
}

@unpublished{salinasDeepARProbabilisticForecasting2019,
  title = {{{DeepAR}}: {{Probabilistic Forecasting}} with {{Autoregressive Recurrent Networks}}},
  shorttitle = {{{DeepAR}}},
  author = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan},
  date = {2019-02-22},
  number = {arXiv:1704.04110},
  eprint = {1704.04110},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1704.04110},
  url = {http://arxiv.org/abs/1704.04110},
  urldate = {2022-10-03},
  abstract = {Probabilistic forecasting, i.e. estimating the probability distribution of a time series' future given its past, is a key enabler for optimizing business processes. In retail businesses, for example, forecasting demand is crucial for having the right inventory available at the right time at the right place. In this paper we propose DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an auto regressive recurrent network model on a large number of related time series. We demonstrate how by applying deep learning techniques to forecasting, one can overcome many of the challenges faced by widely-used classical approaches to the problem. We show through extensive empirical evaluation on several real-world forecasting data sets accuracy improvements of around 15\% compared to state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lss/Zotero/storage/4DI4LNL6/Salinas et al. - 2019 - DeepAR Probabilistic Forecasting with Autoregress.pdf;/Users/lss/Zotero/storage/INCUYKB6/1704.html}
}

@article{sousaLongtermForecastingHourly2022,
  title = {Long-Term Forecasting of Hourly Retail Customer Flow on Intermittent Time Series with Multiple Seasonality},
  author = {Sousa, Martim and Tomé, Ana Maria and Moreira, José},
  date = {2022-09-01},
  journaltitle = {Data Science and Management},
  shortjournal = {Data Science and Management},
  volume = {5},
  number = {3},
  pages = {137--148},
  issn = {2666-7649},
  doi = {10.1016/j.dsm.2022.07.002},
  url = {https://www.sciencedirect.com/science/article/pii/S2666764922000273},
  urldate = {2022-11-02},
  abstract = {In this study, we address a demanding time series forecasting problem that deals simultaneously with the following: (1) intermittent time series, (2) multi-step ahead forecasting, (3) time series with multiple seasonal periods, and (4) performance measures for model selection across multiple time series. Current literature deals with these types of problems separately, and no study has dealt with all these characteristics simultaneously. To fill this knowledge gap, we begin by reviewing all the necessary existing literature relevant to this case study with the goal of proposing a framework capable of achieving adequate forecast accuracy for such a complex problem. Several adaptions and innovations have been conducted, which are marked as contributions to the literature. Specifically, we propose a weighted average forecast combination of many cutting-edge models based on their out-of-sample performance. To gather strong evidence that our ensemble model works in practice, we undertook a large-scale study across 98 time series, rigorously assessed with unbiased performance measures, where a week seasonal naïve was set as a benchmark. The results demonstrate that our proposed ensemble model achieves eye-catching forecasting accuracy.},
  langid = {english},
  keywords = {Multi-step ahead forecasting,Neural networks,Prophet,Scale-independent performance measures,TBATS,Weighted average ensemble},
  file = {/Users/lss/Zotero/storage/N8NNUER2/Sousa et al. - 2022 - Long-term forecasting of hourly retail customer fl.pdf;/Users/lss/Zotero/storage/V6N485RW/S2666764922000273.html}
}

@article{suGDFormerGraphDiffusing2022,
  title = {{{GDFormer}}: {{A Graph Diffusing Attention}} Based Approach for {{Traffic Flow Prediction}}},
  shorttitle = {{{GDFormer}}},
  author = {Su, Jie and Jin, Zhongfu and Ren, Jie and Yang, Jiandang and Liu, Yong},
  date = {2022-04-01},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  volume = {156},
  pages = {126--132},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2022.03.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0167865522000708},
  urldate = {2022-06-29},
  abstract = {In this paper, we propose a novel traffic flow prediction approach, called as Graph Diffusing trans-Former (GDFormer). GDFormer is in architecture of transformer, which is composed by the encoder sequence and decoder sequence. both of the encoder sequence and decoder sequence in GDFormer are constituted by the novel designed Graph Diffusing Attention (GDA) module and the auxiliaries. The GDA module utilizes the query-key-value attention to learn the diffusion parameters for each diffusion step, and dynamically updates the adjacency transition, which reflects the dynamically changing traffic flow between the traffic monitors. To verify the efficiency of our approach, we conduct a lot of experiments on two real-world data sets. With a comparison between our approach and the benchmarks, we find that our approach has achieved state of the art performance. Ablation experiments are conducted to illustrate the effectiveness of the key components in the model. For ease of reproducibility, the code, the processed real-world data sets and the evaluation results are available at https://github.com/dublinsky/GDFormer.},
  langid = {english},
  keywords = {Attention mechanism,Diffusion process,Graph neural network,Traffic flow prediction},
  file = {/Users/lss/Zotero/storage/YKKGU4G2/S0167865522000708.html}
}

@unpublished{taoHierarchicalAttentionBasedRecurrent2018,
  title = {Hierarchical {{Attention-Based Recurrent Highway Networks}} for {{Time Series Prediction}}},
  author = {Tao, Yunzhe and Ma, Lin and Zhang, Weizhong and Liu, Jian and Liu, Wei and Du, Qiang},
  date = {2018-06-02},
  number = {arXiv:1806.00685},
  eprint = {1806.00685},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.00685},
  url = {http://arxiv.org/abs/1806.00685},
  urldate = {2022-06-03},
  abstract = {Time series prediction has been studied in a variety of domains. However, it is still challenging to predict future series given historical observations and past exogenous data. Existing methods either fail to consider the interactions among different components of exogenous variables which may affect the prediction accuracy, or cannot model the correlations between exogenous data and target data. Besides, the inherent temporal dynamics of exogenous data are also related to the target series prediction, and thus should be considered as well. To address these issues, we propose an end-to-end deep learning model, i.e., Hierarchical attention-based Recurrent Highway Network (HRHN), which incorporates spatio-temporal feature extraction of exogenous variables and temporal dynamics modeling of target variables into a single framework. Moreover, by introducing the hierarchical attention mechanism, HRHN can adaptively select the relevant exogenous features in different semantic levels. We carry out comprehensive empirical evaluations with various methods over several datasets, and show that HRHN outperforms the state of the arts in time series prediction, especially in capturing sudden changes and sudden oscillations of time series.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lss/Zotero/storage/JIUFI783/Tao et al. - 2018 - Hierarchical Attention-Based Recurrent Highway Net.pdf;/Users/lss/Zotero/storage/S6EEJU8P/1806.html}
}

@book{taylorForecastingScale2017,
  title = {Forecasting at Scale},
  author = {Taylor, Sean and Letham, Benjamin},
  date = {2017-09-27},
  doi = {10.7287/peerj.preprints.3190v2},
  abstract = {Forecasting is a common data science task that helps organizations with capacity planning, goal setting, and anomaly detection. Despite its importance, there are serious challenges associated with producing reliable and high quality forecasts — especially when there are a variety of time series and analysts with expertise in time series modeling are relatively rare. To address these challenges, we describe a practical approach to forecasting “at scale” that combines configurable models with analyst-in-the-loop performance analysis. We propose a modular regression model with interpretable parameters that can be intuitively adjusted by analysts with domain knowledge about the time series. We describe performance analyses to compare and evaluate forecasting procedures, and automatically flag forecasts for manual review and adjustment. Tools that help analysts to use their expertise most effectively enable reliable, practical forecasting of business time series.},
  file = {/Users/lss/Zotero/storage/28CTDX9T/Taylor and Letham - 2017 - Forecasting at scale.pdf}
}

@unpublished{triebeNeuralProphetExplainableForecasting2021,
  title = {{{NeuralProphet}}: {{Explainable Forecasting}} at {{Scale}}},
  shorttitle = {{{NeuralProphet}}},
  author = {Triebe, Oskar and Hewamalage, Hansika and Pilyugina, Polina and Laptev, Nikolay and Bergmeir, Christoph and Rajagopal, Ram},
  date = {2021-11-29},
  number = {arXiv:2111.15397},
  eprint = {2111.15397},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.15397},
  url = {http://arxiv.org/abs/2111.15397},
  urldate = {2022-09-19},
  abstract = {We introduce NeuralProphet, a successor to Facebook Prophet, which set an industry standard for explainable, scalable, and user-friendly forecasting frameworks. With the proliferation of time series data, explainable forecasting remains a challenging task for business and operational decision making. Hybrid solutions are needed to bridge the gap between interpretable classical methods and scalable deep learning models. We view Prophet as a precursor to such a solution. However, Prophet lacks local context, which is essential for forecasting the near-term future and is challenging to extend due to its Stan backend. NeuralProphet is a hybrid forecasting framework based on PyTorch and trained with standard deep learning methods, making it easy for developers to extend the framework. Local context is introduced with auto-regression and covariate modules, which can be configured as classical linear regression or as Neural Networks. Otherwise, NeuralProphet retains the design philosophy of Prophet and provides the same basic model components. Our results demonstrate that NeuralProphet produces interpretable forecast components of equivalent or superior quality to Prophet on a set of generated time series. NeuralProphet outperforms Prophet on a diverse collection of real-world datasets. For short to medium-term forecasts, NeuralProphet improves forecast accuracy by 55 to 92 percent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lss/Zotero/storage/B6I5T9D8/Triebe et al. - 2021 - NeuralProphet Explainable Forecasting at Scale.pdf;/Users/lss/Zotero/storage/Y4LQ99Z9/2111.html}
}

@misc{turkmenIntermittentDemandForecasting2019,
  title = {Intermittent {{Demand Forecasting}} with {{Deep Renewal Processes}}},
  author = {Turkmen, Ali Caner and Wang, Yuyang and Januschowski, Tim},
  date = {2019-11-23},
  number = {arXiv:1911.10416},
  eprint = {1911.10416},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1911.10416},
  urldate = {2022-11-15},
  abstract = {Intermittent demand, where demand occurrences appear sporadically in time, is a common and challenging problem in forecasting. In this paper, we first make the connections between renewal processes, and a collection of current models used for intermittent demand forecasting. We then develop a set of models that benefit from recurrent neural networks to parameterize conditional interdemand time and size distributions, building on the latest paradigm in "deep" temporal point processes. We present favorable empirical findings on discrete and continuous time intermittent demand data, validating the practical value of our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lss/Zotero/storage/J6RPHLHT/Turkmen et al. - 2019 - Intermittent Demand Forecasting with Deep Renewal .pdf;/Users/lss/Zotero/storage/NTK9TG6R/1911.html}
}

@article{valdes-donosoUsingMachineLearning2017,
  title = {Using {{Machine Learning}} to {{Predict Swine Movements}} within a {{Regional Program}} to {{Improve Control}} of {{Infectious Diseases}} in the {{US}}},
  author = {Valdes-Donoso, Pablo and VanderWaal, Kimberly and Jarvis, Lovell S. and Wayne, Spencer R. and Perez, Andres M.},
  date = {2017},
  journaltitle = {Frontiers in Veterinary Science},
  volume = {4},
  issn = {2297-1769},
  url = {https://www.frontiersin.org/article/10.3389/fvets.2017.00002},
  urldate = {2022-02-25},
  abstract = {Between-farm animal movement is one of the most important factors influencing the spread of infectious diseases in food animals, including in the US swine industry. Understanding the structural network of contacts in a food animal industry is prerequisite to planning for efficient production strategies and for effective disease control measures. Unfortunately, data regarding between-farm animal movements in the US are not systematically collected and thus, such information is often unavailable. In this paper, we develop a procedure to replicate the structure of a network, making use of partial data available, and subsequently use the model developed to predict animal movements among sites in 34 Minnesota counties. First, we summarized two networks of swine producing facilities in Minnesota, then we used a machine learning technique referred to as random forest, an ensemble of independent classification trees, to estimate the probability of pig movements between farms and/or markets sites located in two counties in Minnesota. The model was calibrated and tested by comparing predicted data and observed data in those two counties for which data were available. Finally, the model was used to predict animal movements in sites located across 34 Minnesota counties. Variables that were important in predicting pig movements included between-site distance, ownership, and production type of the sending and receiving farms and/or markets. Using a weighted-kernel approach to describe spatial variation in the centrality measures of the predicted network, we showed that the south-central region of the study area exhibited high aggregation of predicted pig movements. Our results show an overlap with the distribution of outbreaks of porcine reproductive and respiratory syndrome, which is believed to be transmitted, at least in part, though animal movements. While the correspondence of movements and disease is not a causal test, it suggests that the predicted network may approximate actual movements. Accordingly, the predictions provided here might help to design and implement control strategies in the region. Additionally, the methodology here may be used to estimate contact networks for other livestock systems when only incomplete information regarding animal movements is available.},
  file = {/Users/lss/Zotero/storage/32C94CC8/Valdes-Donoso et al. - 2017 - Using Machine Learning to Predict Swine Movements .pdf}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-10-05},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/lss/Zotero/storage/QK6R7XZT/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/lss/Zotero/storage/8X9HJPNV/1706.html}
}

@inproceedings{wuConnectingDotsMultivariate2020,
  title = {Connecting the {{Dots}}: {{Multivariate Time Series Forecasting}} with {{Graph Neural Networks}}},
  shorttitle = {Connecting the {{Dots}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Wu, Zonghan and Pan, Shirui and Long, Guodong and Jiang, Jing and Chang, Xiaojun and Zhang, Chengqi},
  date = {2020-08-23},
  pages = {753--763},
  publisher = {{ACM}},
  location = {{Virtual Event CA USA}},
  doi = {10.1145/3394486.3403118},
  url = {https://dl.acm.org/doi/10.1145/3394486.3403118},
  urldate = {2022-07-21},
  abstract = {Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it is fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.},
  eventtitle = {{{KDD}} '20: {{The}} 26th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-1-4503-7998-4},
  langid = {english},
  file = {/Users/lss/Zotero/storage/MUH76KZP/Wu et al. - 2020 - Connecting the Dots Multivariate Time Series Fore.pdf}
}

@misc{wuDeepTransformerModels2020,
  title = {Deep {{Transformer Models}} for {{Time Series Forecasting}}: {{The Influenza Prevalence Case}}},
  shorttitle = {Deep {{Transformer Models}} for {{Time Series Forecasting}}},
  author = {Wu, Neo and Green, Bradley and Ben, Xue and O'Banion, Shawn},
  date = {2020-01-22},
  number = {arXiv:2001.08317},
  eprint = {2001.08317},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.08317},
  url = {http://arxiv.org/abs/2001.08317},
  urldate = {2022-10-05},
  abstract = {In this paper, we present a new approach to time series forecasting. Time series data are prevalent in many scientific and engineering disciplines. Time series forecasting is a crucial task in modeling time series data, and is an important area of machine learning. In this work we developed a novel method that employs Transformer-based machine learning models to forecast time series data. This approach works by leveraging self-attention mechanisms to learn complex patterns and dynamics from time series data. Moreover, it is a generic framework and can be applied to univariate and multivariate time series data, as well as time series embeddings. Using influenza-like illness (ILI) forecasting as a case study, we show that the forecasting results produced by our approach are favorably comparable to the state-of-the-art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lss/Zotero/storage/WMQ5KIHU/Wu et al. - 2020 - Deep Transformer Models for Time Series Forecastin.pdf;/Users/lss/Zotero/storage/YBKIKIBW/2001.html}
}

@unpublished{yueTS2VecUniversalRepresentation2022,
  title = {{{TS2Vec}}: {{Towards Universal Representation}} of {{Time Series}}},
  shorttitle = {{{TS2Vec}}},
  author = {Yue, Zhihan and Wang, Yujing and Duan, Juanyong and Yang, Tianmeng and Huang, Congrui and Tong, Yunhai and Xu, Bixiong},
  date = {2022-02-03},
  number = {arXiv:2106.10466},
  eprint = {2106.10466},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.10466},
  url = {http://arxiv.org/abs/2106.10466},
  urldate = {2022-08-16},
  abstract = {This paper presents TS2Vec, a universal framework for learning representations of time series in an arbitrary semantic level. Unlike existing methods, TS2Vec performs contrastive learning in a hierarchical way over augmented context views, which enables a robust contextual representation for each timestamp. Furthermore, to obtain the representation of an arbitrary sub-sequence in the time series, we can apply a simple aggregation over the representations of corresponding timestamps. We conduct extensive experiments on time series classification tasks to evaluate the quality of time series representations. As a result, TS2Vec achieves significant improvement over existing SOTAs of unsupervised time series representation on 125 UCR datasets and 29 UEA datasets. The learned timestamp-level representations also achieve superior results in time series forecasting and anomaly detection tasks. A linear regression trained on top of the learned representations outperforms previous SOTAs of time series forecasting. Furthermore, we present a simple way to apply the learned representations for unsupervised anomaly detection, which establishes SOTA results in the literature. The source code is publicly available at https://github.com/yuezhihan/ts2vec.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/lss/Zotero/storage/B86R83IU/Yue et al. - 2022 - TS2Vec Towards Universal Representation of Time S.pdf;/Users/lss/Zotero/storage/Z2ZW7CCE/2106.html}
}

@unpublished{zhangLessMoreFast2022,
  title = {Less {{Is More}}: {{Fast Multivariate Time Series Forecasting}} with {{Light Sampling-oriented MLP Structures}}},
  shorttitle = {Less {{Is More}}},
  author = {Zhang, Tianping and Zhang, Yizhuo and Cao, Wei and Bian, Jiang and Yi, Xiaohan and Zheng, Shun and Li, Jian},
  date = {2022-07-04},
  number = {arXiv:2207.01186},
  eprint = {2207.01186},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.01186},
  url = {http://arxiv.org/abs/2207.01186},
  urldate = {2022-08-16},
  abstract = {Multivariate time series forecasting has seen widely ranging applications in various domains, including finance, traffic, energy, and healthcare. To capture the sophisticated temporal patterns, plenty of research studies designed complex neural network architectures based on many variants of RNNs, GNNs, and Transformers. However, complex models are often computationally expensive and thus face a severe challenge in training and inference efficiency when applied to large-scale real-world datasets. In this paper, we introduce LightTS, a light deep learning architecture merely based on simple MLP-based structures. The key idea of LightTS is to apply an MLP-based structure on top of two delicate down-sampling strategies, including interval sampling and continuous sampling, inspired by a crucial fact that down-sampling time series often preserves the majority of its information. We conduct extensive experiments on eight widely used benchmark datasets. Compared with the existing state-of-the-art methods, LightTS demonstrates better performance on five of them and comparable performance on the rest. Moreover, LightTS is highly efficient. It uses less than 5\% FLOPS compared with previous SOTA methods on the largest benchmark dataset. In addition, LightTS is robust and has a much smaller variance in forecasting accuracy than previous SOTA methods in long sequence forecasting tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/lss/Zotero/storage/MFP3Z9HR/Zhang et al. - 2022 - Less Is More Fast Multivariate Time Series Foreca.pdf;/Users/lss/Zotero/storage/YH8GB8VN/2207.html}
}

@misc{zhouInformerEfficientTransformer2021,
  title = {Informer: {{Beyond Efficient Transformer}} for {{Long Sequence Time-Series Forecasting}}},
  shorttitle = {Informer},
  author = {Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  date = {2021-03-28},
  number = {arXiv:2012.07436},
  eprint = {2012.07436},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.07436},
  url = {http://arxiv.org/abs/2012.07436},
  urldate = {2022-10-05},
  abstract = {Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a \$ProbSparse\$ self-attention mechanism, which achieves \$O(L \textbackslash log L)\$ in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/lss/Zotero/storage/KEYAPPT9/Zhou et al. - 2021 - Informer Beyond Efficient Transformer for Long Se.pdf;/Users/lss/Zotero/storage/HMCBVVIU/2012.html}
}

@unpublished{zhuASTGCNAttributeAugmentedSpatiotemporal2020,
  title = {{{AST-GCN}}: {{Attribute-Augmented Spatiotemporal Graph Convolutional Network}} for {{Traffic Forecasting}}},
  shorttitle = {{{AST-GCN}}},
  author = {Zhu, Jiawei and Tao, Chao and Deng, Hanhan and Zhao, Ling and Wang, Pu and Lin, Tao and Li, Haifeng},
  date = {2020-11-22},
  eprint = {2011.11004},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2011.11004},
  urldate = {2022-03-07},
  abstract = {Traffic forecasting is a fundamental and challenging task in the field of intelligent transportation. Accurate forecasting not only depends on the historical traffic flow information but also needs to consider the influence of a variety of external factors, such as weather conditions and surrounding POI distribution. Recently, spatiotemporal models integrating graph convolutional networks and recurrent neural networks have become traffic forecasting research hotspots and have made significant progress. However, few works integrate external factors. Therefore, based on the assumption that introducing external factors can enhance the spatiotemporal accuracy in predicting traffic and improving interpretability, we propose an attribute-augmented spatiotemporal graph convolutional network (AST-GCN). We model the external factors as dynamic attributes and static attributes and design an attribute-augmented unit to encode and integrate those factors into the spatiotemporal graph convolution model. Experiments on real datasets show the effectiveness of considering external information on traffic forecasting tasks when compared to traditional traffic prediction methods. Moreover, under different attribute-augmented schemes and prediction horizon settings, the forecasting accuracy of the AST-GCN is higher than that of the baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/lss/Zotero/storage/9RAV8D49/Zhu et al. - 2020 - AST-GCN Attribute-Augmented Spatiotemporal Graph .pdf;/Users/lss/Zotero/storage/2UAQZEFA/2011.html}
}
